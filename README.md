üó£Ô∏è Hidden Markov Models & Bayesian Estimation for Dysarthric Speech Recognition!(https://www.google.com/search?q=https://img.shields.io/badge/Status-Completed-success)A comparative study of Generative (HMM, Bayesian GMM) vs. Discriminative (Random Forest) models for classifying pathological speech using acoustic feature engineering and PCA clustering.üìã Table of ContentsAbstractKey Features-(#-system-architecture)-(#-datasets)Installation & UsageMethodology-(#-results--analysis)-(#torgo-dataset-results)-(#rawdyspeech-dataset-results)Comparative PerformanceContributorsüìÑ AbstractDysarthria is a motor speech disorder resulting in poor articulation and reduced intelligibility. Standard speech recognition systems often fail on such atypical speech due to high intra-speaker variability. This project implements a robust pattern recognition pipeline to classify Dysarthric vs. Control (Healthy) speech.We bridge the "Perceptual-Statistical Gap" by comparing:Hidden Markov Models (HMM): For temporal sequence modeling.Bayesian Gaussian Mixture Models (BGMM): For probabilistic clustering with Dirichlet Process Priors.Random Forest (RF): For non-linear discriminative classification.Our results demonstrate that Random Forest achieves state-of-the-art accuracy (~88%) by effectively handling non-linear feature interactions in a PCA-reduced space, while Bayesian GMMs offer superior robustness against overfitting in data-scarce scenarios.üöÄ Key FeaturesHybrid Feature Extraction: Extracts MFCCs (spectral envelope), F0 (pitch), and Formants (F1-F3) to capture articulatory blurring and monopitch.Dimensionality Reduction: Uses Principal Component Analysis (PCA) to retain 95% variance, eliminating multicollinearity for probabilistic models.Statistical Validation: Implements Welch's t-test to statistically validate feature differences between healthy and pathological groups ($p < 0.05$).Advanced Modeling:Bayesian GMM with Dirichlet Process priors for automatic component selection.Random Forest with balanced class weighting for robust decision boundaries.‚öôÔ∏è System ArchitectureThe project follows a linear processing pipeline designed for modularity and scalability:Input Layer: Raw Audio (.wav) ingestion.Preprocessing: Resampling (16kHz), Normalization, Silence Removal.Feature Extraction: Librosa-based extraction of MFCCs, Pitch (YIN), and LPC Formants.Transformation: PCA for orthogonalization and noise reduction.Classification: Parallel training of HMM, BGMM, and RF models.üíæ DatasetsThe project utilizes two distinct datasets to validate model generalization:DatasetDescriptionSamplesCharacteristicsTORGOAcoustic & Articulatory data from speakers with Cerebral Palsy / ALS.~17,600High Class Imbalance, Variable Noise Conditions.RAWDysPeechPre-processed raw waveforms (derived from UASpeech).~160,000Balanced Classes, High Sampling Rate (44.1kHz).üõ† Installation & UsagePrerequisitesEnsure you have Python 3.9+ installed.1. Clone the Repositorybashgit clone https://github.com/your-username/dysarthric-speech-recognition.gitcd dysarthric-speech-recognition
### 2. Install Dependencies
```bash
pip install -r requirements.txt
Key libraries: librosa, numpy, pandas, scikit-learn, hmmlearn, matplotlib, seaborn, joblib.3. Run the PipelinePlace your datasets in the data/ folder and run the main notebook:Bashjupyter notebook Dysarthric_Speech_Pipeline.ipynb
üî¨ Methodology1. Feature ExtractionWe extract a 17-dimensional feature vector for every audio frame:MFCCs (1-13): Represents the shape of the vocal tract. Dysarthric speech often shows "blurred" MFCC trajectories.F0 (Fundamental Frequency): Detects "monopitch" (low variance) common in Parkinsonian dysarthria.Formants (F1-F3): Detects "vowel centralization" (reduced articulatory space).2. Dimensionality Reduction (PCA)The correlation matrix revealed high multicollinearity between MFCC coefficients. We applied PCA to project the data into an orthogonal space, retaining 95% of the variance. This is crucial for the Bayesian GMM to function correctly (as it assumes diagonal covariance).3. Modeling StrategyDiscriminative: Random Forest (100 trees) to learn complex decision boundaries.Generative: Bayesian GMM to model the probability density of "healthy" vs "pathological" speech using priors to prevent overfitting.Temporal: HMM to capture the sequence of speech states (onset -> nucleus -> coda).üìä Results & AnalysisTORGO Dataset ResultsClass Distribution & Feature Analysis<div align="center"><img src="results/torgo_class_dist.png" width="45%" alt="Class Distribution"><img src="results/torgo_features.png" width="45%" alt="Feature Distributions"></div>Observation: The dataset is heavily imbalanced (2:1 Control vs. Dysarthric). The boxplots (right) show significant variance in F0 and MFCCs, validating them as discriminative features.Model Performance (ROC & Confusion Matrix)<div align="center"><img src="results/torgo_roc.png" width="45%" alt="ROC Curve"><img src="results/torgo_conf_matrix.png" width="45%" alt="Confusion Matrix"></div>Analysis: The Random Forest achieves an outstanding AUC of 0.94, indicating excellent separability. The Confusion Matrix shows it has the highest True Positive rate, minimizing false negatives which is critical for medical diagnosis.RAWDysPeech Dataset ResultsClass Balance & ROC Analysis<div align="center"><img src="results/rawdysspeech_class_dist.png" width="45%" alt="RAWDysPeech Class Dist"><img src="results/rawdysspeech_roc.png" width="45%" alt="RAWDysPeech ROC"></div>Analysis: RAWDysPeech is perfectly balanced. The Random Forest maintains its dominance with an AUC of 0.95, proving the pipeline scales well to larger datasets.Comparative PerformanceModelTORGO AccuracyRAWDysPeech AccuracyKey StrengthRandom Forest88.18%87.81%Handles non-linear boundaries & outliers best.Bayesian GMM84.17%83.17%Robust to overfitting via probabilistic priors.HMM64.68%62.79%Captures temporal evolution (limited by static features).Conclusion: Discriminative models (Random Forest) significantly outperform standard Generative models (HMM) for this task, likely due to the static nature of the aggregated feature vectors. However, Bayesian GMMs provide a strong alternative for unsupervised clustering or data-scarce scenarios.üë• ContributorsAbhinav Saluja (BL.EN.U4CSE22002)Amitesh Panda (BL.EN.U4CSE22010)Suhas Kesavan (BL.EN.U4CSE22060)Under the guidance of Dr. Amulyashree S, Amrita School of Computing.üìú LicenseThis project is licensed under the MIT License - see the(LICENSE) file for details.
